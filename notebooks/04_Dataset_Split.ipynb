{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"#Setup\" data-toc-modified-id=\"Setup-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Setup</a></span></li><li><span><a href=\"#Libraries\" data-toc-modified-id=\"Libraries-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Libraries</a></span></li><li><span><a href=\"#Data\" data-toc-modified-id=\"Data-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Data</a></span></li><li><span><a href=\"#Split\" data-toc-modified-id=\"Split-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Split</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook splits the dataset into a train, validation and test split:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import data\n",
    "import pyspark.sql.functions as f\n",
    "from lenskit import crossfold as xf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+-------------------+\n",
      "|userId|movieId|rating|          timestamp|\n",
      "+------+-------+------+-------------------+\n",
      "|     1|      2|   3.5|2005-04-02 23:53:47|\n",
      "|     1|     29|   3.5|2005-04-02 23:31:16|\n",
      "|     1|     32|   3.5|2005-04-02 23:33:39|\n",
      "|     1|     47|   3.5|2005-04-02 23:32:07|\n",
      "|     1|     50|   3.5|2005-04-02 23:29:40|\n",
      "+------+-------+------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = data.get_data(\"/tmp/ml-20m/ratings.csv\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+------+-------------------+\n",
      "|user|item|rating|          timestamp|\n",
      "+----+----+------+-------------------+\n",
      "|   1|   2|   3.5|2005-04-02 23:53:47|\n",
      "|   1|  29|   3.5|2005-04-02 23:31:16|\n",
      "|   1|  32|   3.5|2005-04-02 23:33:39|\n",
      "|   1|  47|   3.5|2005-04-02 23:32:07|\n",
      "|   1|  50|   3.5|2005-04-02 23:29:40|\n",
      "+----+----+------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df \\\n",
    "    .withColumnRenamed(\"userId\", \"user\") \\\n",
    "    .withColumnRenamed(\"movieId\", \"item\") \\\n",
    "    .cache()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check timestamp range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n",
      "|     min(timestamp)|     max(timestamp)|\n",
      "+-------------------+-------------------+\n",
      "|1995-01-09 11:46:44|2015-03-31 06:40:02|\n",
      "+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\"min(timestamp)\", \"max(timestamp)\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training on whole dataset may take too long so we filter by timestamp to reduce dataset size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------+-------------------+\n",
      "|user| item|rating|          timestamp|\n",
      "+----+-----+------+-------------------+\n",
      "|  11| 4226|   5.0|2011-01-12 01:35:59|\n",
      "|  11| 5971|   5.0|2011-01-12 01:36:41|\n",
      "|  11| 6291|   5.0|2011-01-12 01:35:13|\n",
      "|  11| 7153|   5.0|2011-01-12 01:35:32|\n",
      "|  11|30707|   5.0|2011-01-12 01:36:16|\n",
      "+----+-----+------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.filter(f.expr(\"timestamp >= '2010-01-01'\"))\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 13min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_df, test_df = next(xf.partition_users(df.toPandas(), 1, xf.SampleFrac(0.2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in training set: 3,078,276\n",
      "Number of rows in test set: 769,271\n"
     ]
    }
   ],
   "source": [
    "nrows_train, _ = train_df.shape\n",
    "nrows_test, _ = test_df.shape\n",
    "\n",
    "print(f\"Number of rows in training set: {nrows_train:,d}\")\n",
    "print(f\"Number of rows in test set: {nrows_test:,d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    " spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 21min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# df.write.format(source).mode(\"overwrite\").save(path)\n",
    "spark.createDataFrame(train_df).write.mode(\"overwrite\").parquet(\"/tmp/ml-20m/train_df.parquet\")\n",
    "spark.createDataFrame(test_df).write.mode(\"overwrite\").parquet(\"/tmp/ml-20m/test_df.parquet\")"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "dbconnect",
   "language": "python",
   "name": "dbconnect"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
